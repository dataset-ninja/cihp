To benchmark the more challenging multi-person human parsing task, authors build a large-scale dataset called Crowd Instance-level Human Parsing (**CIHP**) dataset, which has several appealing properties. First, with 38,280 diverse human images, it is the largest multi-person human parsing dataset to date. Second, CIHP is annotated with rich information of person items. The images in this dataset are labeled with pixel-wise annotations on 20 categories and instance-level identification. Third, the images collected from the real-world scenarios contain people appearing with challenging poses and viewpoints, heavy occlusions, various appearances and in a wide range of resolutions. Some examples are shown in Figure bellow. With the CIHP dataset, authors propose a new benchmark for instance-level human parsing together with a standard evaluation server where the test set will be kept secret to avoid overfitting.

![Fig](https://i.ibb.co/dkJqhXN/Screenshot-2023-09-29-131552.png)
<i> Left: Statistics on the number of persons in one image. Right: The data distribution on 19 semantic part labels in the CIHP dataset.</i>

The images in the CIHP are collected from unconstrained resources like Google and Bing. authors manually specify several keywords (e.g., family, couple, party, meeting, etc.) to gain a great diversity of multi-person images. The crawled images are elaborately annotated by a professional labeling organization with well quality control. Authors supervise the whole annotation process and conduct a second-round check for each annotated image. Authors remove the unusable images that are of low resolution, image quality, or contain one or no person instance. In total, 38,280 images are kept to construct the CIHP dataset. Following random selection, authors arrive at a unique split that consists of 28,280 training and 5,000 validation images with publicly available annotations, as well as 5,000 test images with annotations withheld for benchmarking purposes.

Authors now introduce the images and categories in the CIHP dataset with more statistical details. Superior to the [previous attempts](https://ieeexplore.ieee.org/document/7410520) with average one or two person instances in an image, all images of the CIHP dataset contain two or more instances with an average of <i>3.4</i>. Generally, authors follow LIP [11] to define and annotate the semantic part labels. However, they find that the Jumpsuit label defined in [LIP](https://arxiv.org/abs/1703.05446) is infrequent compared to other labels. To parse the human more completely and precisely, authors use a more common body part label (Tosor-skin) instead. The 19 semantic part labels in the CIHP are *hat*, *hair*, *sunglasses*, *upper-clothes*, *dress*, *coat*, *socks*, *pants*, *gloves*, *scarf*, *skirt*, *torsoskin*, *face*, *right_arm*, *left_arm*, *right_leg*, *left_leg*, *right_shoe* and *left_shoe*. 
